{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# RNN(Recurrent Neural Networks)\n",
    "### RNNs are a type of neural network which is used basically for sequential data. Especially when the order of the data points plays an import role. \n",
    "### One big example is time series, where on the basis of previous data one has to predict the output value of future time step. Apart from that, automated text generation by analyzing the sequence. Can be used for images but CNNs are preferred having better conclusions with spatial data, anyways any spatial data is also a sequential data.\n",
    "### Input to RNN at each time step contains 2 things:\n",
    "#### 1) Current state values/vectors\n",
    "#### 2) State vector is an encoded memory containing the learnings from previous time steps(for initial state it's set to 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Steps:\n",
    "### 1) Gather data\n",
    "### 2) input data = batch_size * features * time steps\n",
    "### 3) arrange them according to different time steps i.e. for each time step batch_size*features\n",
    "### 4) Set state of dimensions batch_size*state_size\n",
    "### 5) Make initial state's matrix values 0\n",
    "### 6) For each time step concatenate current state and current input by columns to obtain the mix of current input data and past learnings\n",
    "### 7) Step 6 in a loop where compute logits with first layer weights and squash them to non linearity with tanh or softmax to obtain the new state ahead\n",
    "### 8) In step 7, save the states obtained in a list and update current state with the new state obtained by above process in every iteration\n",
    "### 9) Once the hidden layer set is over.\n",
    "### 10) Take the final layer of each RNN units and subject them to softmax separately to obtain prediction probability\n",
    "### 11) Calculate separate losses for each RNN units\n",
    "### 12) Backprop the total average loss using Adagrad / Gradient Descent etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Step 0 : Load dependencies\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic flowcharts showing RNN structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/max/1600/1*UkI9za9zTR-HL8uM15Wmzw.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://cdn-images-1.medium.com/max/1600/1*UkI9za9zTR-HL8uM15Wmzw.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://karpathy.github.io/assets/rnn/charseq.jpeg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"http://karpathy.github.io/assets/rnn/charseq.jpeg\")   #very intuitive with example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Step - 1 : hyperparameters\n",
    "num_epochs = 100 #number of iterations/learnings where each epoch would be subjected to a new batch\n",
    "total_series_length = 50000\n",
    "truncated_backprop_length = 15\n",
    "state_size = 4 #number of neurons in our hidden layer\n",
    "num_classes = 1\n",
    "echo_step = 3\n",
    "batch_size = 5\n",
    "num_batches = total_series_length//batch_size//truncated_backprop_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Step - 2 : Collect data(here:generate data)\n",
    "def generateData():\n",
    "    #0,1, 50K samples, 50% chance each chosen\n",
    "    x = np.random.choice(2,total_series_length,p=[0.5,0.5])\n",
    "    #x = np.random.choice(2,10,p=[0.5,0.5])\n",
    "    y = np.roll(x,echo_step)  #shift echo_step steps to the right\n",
    "    x = x.reshape([batch_size,-1])\n",
    "    y = y.reshape([batch_size,-1])\n",
    "    \n",
    "    return (x,y)\n",
    "\n",
    "data = generateData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape #x shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1].shape #y shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Step- 3 : Build the Model\n",
    "batchX_placeholder = tf.placeholder(tf.float32,[batch_size,truncated_backprop_length])\n",
    "batchY_placeholder = tf.placeholder(tf.int32,[batch_size,truncated_backprop_length])\n",
    "#Also the RNN-state is supplied in a placeholder, \n",
    "#which is saved from the output of the previous run \n",
    "#this state placeholder is the key \n",
    "init_state = tf.placeholder(tf.float32,[batch_size,state_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#declare weights and biases\n",
    "#here: 3 layer recurrent neural net with 1 hidden layer\n",
    "w = tf.Variable(np.random.rand(state_size+1,state_size),dtype=tf.float32)\n",
    "b = tf.Variable(np.zeros((1,state_size)),dtype=tf.float32)\n",
    "\n",
    "w2 = tf.Variable(np.random.rand(state_size,num_classes),dtype=tf.float32)\n",
    "b2 = tf.Variable(np.zeros((1,num_classes)),dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start building the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first split the data into adjacent time steps\n",
    "# unpack columns (each array for a column)\n",
    "input_series = tf.unstack(batchX_placeholder,axis = 1)\n",
    "label_series = tf.unstack(batchY_placeholder,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#forward pass\n",
    "#state placeholder\n",
    "current_state = init_state\n",
    "#series of states through time\n",
    "states_series = []\n",
    "\n",
    "\n",
    "#for each set of inputs\n",
    "#forward pass through the network to get new state value\n",
    "#store all states in memory\n",
    "for current_input in input_series:\n",
    "    current_input = tf.reshape(current_input,[batch_size,1])  #format the input\n",
    "    input_and_state_concatenated = tf.concat(axis=1,values=[current_input,current_state]) \n",
    "    #above mixing input and state data therefore increase in number of columns\n",
    "    \n",
    "    #now perform matrix multiplication between weights and input, add bias\n",
    "    #squash with a nonlinearity, for probabiolity value\n",
    "    next_state = tf.nn.tanh(tf.matmul(input_and_state_concatenated,w)+b)\n",
    "    states_series.append(next_state)\n",
    "    current_state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/max/1600/1*fdwNNJ5UOE3Sx0R_Cyfmyg.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://cdn-images-1.medium.com/max/1600/1*fdwNNJ5UOE3Sx0R_Cyfmyg.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may wonder the variable name truncated_backprop_length is supposed to mean. When a RNN is trained, it is actually treated as a deep neural network with reoccurring weights in every layer. These layers will not be unrolled to the beginning of time, that would be too computationally expensive, and are therefore truncated at a limited number of time-steps. In our sample schematics above, the error is backpropagated three steps in our batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#calculate loss and minimize it\n",
    "\n",
    "#calculate loss\n",
    "#second part of forward pass\n",
    "#logits short for logistic transform\n",
    "logit_series = [tf.matmul(x,w2)+b2 for x in states_series]\n",
    "\n",
    "#apply softmax nonlinearity for output probability\n",
    "predictions_series = [tf.nn.softmax(logits) for logits in logit_series]\n",
    "\n",
    "label_series = [tf.reshape(label,[5,1]) for label in label_series]\n",
    "\n",
    "#measure loss, calculate softmax again on logits, then compute cross entropy\n",
    "#measures the difference between two probability distributions\n",
    "#this will return A Tensor of the same shape as labels and of the same type as logits \n",
    "#with the softmax cross entropy loss.\n",
    "losses = [tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=labels) for logits,labels in zip(logit_series,label_series)]\n",
    "\n",
    "#computes average, one value\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "\n",
    "#use adagrad to minimize with .3 learning rate\n",
    "#minimize it with adagrad, not SGD\n",
    "#One downside of SGD is that it is sensitive to\n",
    "#the learning rate hyper-parameter. When the data are sparse and features have\n",
    "#different frequencies, a single learning rate for every weight update can have\n",
    "#exponential regret.\n",
    "#Some features can be extremely useful and informative to an optimization problem but \n",
    "#they may not show up in most of the training instances or data. If, when they do show up, \n",
    "#they are weighted equally in terms of learning rate as a feature that has shown up hundreds \n",
    "#of times we are practically saying that the influence of such features means nothing in the \n",
    "#overall optimization. it's impact per step in the stochastic gradient descent will be so small \n",
    "#that it can practically be discounted). To counter this, AdaGrad makes it such that features \n",
    "#that are more sparse in the data have a higher learning rate which translates into a larger \n",
    "#update for that feature\n",
    "#sparse features can be very useful.\n",
    "#Each feature has a different learning rate which is adaptable. \n",
    "#gives voice to the little guy who matters a lot\n",
    "#weights that receive high gradients will have their effective learning rate reduced, \n",
    "#while weights that receive small or infrequent updates will have their effective learning rate increased. \n",
    "#great paper http://seed.ucsd.edu/mediawiki/images/6/6a/Adagrad.pdf\n",
    "train_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "WARNING:tensorflow:From <ipython-input-16-9317531f58eb>:6: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f67480dd790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New data, epoch 0\n",
      "Step 0 Loss 0.0\n",
      "Step 100 Loss 0.0\n",
      "Step 200 Loss 0.0\n",
      "Step 300 Loss 0.0\n",
      "Step 400 Loss 0.0\n",
      "Step 500 Loss 0.0\n",
      "Step 600 Loss 0.0\n",
      "New data, epoch 1\n",
      "Step 0 Loss 0.0\n",
      "Step 100 Loss 0.0\n",
      "Step 200 Loss 0.0\n",
      "Step 300 Loss 0.0\n",
      "Step 400 Loss 0.0\n",
      "Step 500 Loss 0.0\n",
      "Step 600 Loss 0.0\n",
      "New data, epoch 2\n",
      "Step 0 Loss 0.0\n",
      "Step 100 Loss 0.0\n",
      "Step 200 Loss 0.0\n",
      "Step 300 Loss 0.0\n",
      "Step 400 Loss 0.0\n",
      "Step 500 Loss 0.0\n",
      "Step 600 Loss 0.0\n",
      "New data, epoch 3\n",
      "Step 0 Loss 0.0\n",
      "Step 100 Loss 0.0\n",
      "Step 200 Loss 0.0\n",
      "Step 300 Loss 0.0\n",
      "Step 400 Loss 0.0\n",
      "Step 500 Loss 0.0\n",
      "Step 600 Loss 0.0\n",
      "New data, epoch 4\n",
      "Step 0 Loss 0.0\n",
      "Step 100 Loss 0.0\n",
      "Step 200 Loss 0.0\n",
      "Step 300 Loss 0.0\n",
      "Step 400 Loss 0.0\n",
      "Step 500 Loss 0.0\n",
      "Step 600 Loss 0.0\n",
      "New data, epoch 5\n",
      "Step 0 Loss 0.0\n",
      "Step 100 Loss 0.0\n",
      "Step 200 Loss 0.0\n",
      "Step 300 Loss 0.0\n",
      "Step 400 Loss 0.0\n",
      "Step 500 Loss 0.0\n",
      "Step 600 Loss 0.0\n",
      "New data, epoch 6\n",
      "Step 0 Loss 0.0\n",
      "Step 100 Loss 0.0\n",
      "Step 200 Loss 0.0\n",
      "Step 300 Loss 0.0\n",
      "Step 400 Loss 0.0\n",
      "Step 500 Loss 0.0\n",
      "Step 600 Loss 0.0\n",
      "New data, epoch 7\n",
      "Step 0 Loss 0.0\n",
      "Step 100 Loss 0.0\n",
      "Step 200 Loss 0.0\n",
      "Step 300 Loss 0.0\n",
      "Step 400 Loss 0.0\n",
      "Step 500 Loss 0.0\n",
      "Step 600 Loss 0.0\n",
      "New data, epoch 8\n",
      "Step 0 Loss 0.0\n",
      "Step 100 Loss 0.0\n",
      "Step 200 Loss 0.0\n",
      "Step 300 Loss 0.0\n",
      "Step 400 Loss 0.0\n",
      "Step 500 Loss 0.0\n",
      "Step 600 Loss 0.0\n",
      "New data, epoch 9\n",
      "Step 0 Loss 0.0\n",
      "Step 100 Loss 0.0\n",
      "Step 200 Loss 0.0\n",
      "Step 300 Loss 0.0\n",
      "Step 400 Loss 0.0\n",
      "Step 500 Loss 0.0\n",
      "Step 600 Loss 0.0\n",
      "New data, epoch 10\n",
      "Step 0 Loss 0.0\n",
      "Step 100 Loss 0.0\n",
      "Step 200 Loss 0.0\n",
      "Step 300 Loss 0.0\n",
      "Step 400 Loss 0.0\n",
      "Step 500 Loss 0.0\n",
      "Step 600 Loss 0.0\n",
      "New data, epoch 11\n",
      "Step 0 Loss 0.0\n",
      "Step 100 Loss 0.0\n",
      "Step 200 Loss 0.0\n",
      "Step 300 Loss 0.0\n",
      "Step 400 Loss 0.0\n",
      "Step 500 Loss 0.0\n",
      "Step 600 Loss 0.0\n",
      "New data, epoch 12\n",
      "Step 0 Loss 0.0\n",
      "Step 100 Loss 0.0\n",
      "Step 200 Loss 0.0\n",
      "Step 300 Loss 0.0\n",
      "Step 400 Loss 0.0\n",
      "Step 500 Loss 0.0\n",
      "Step 600 Loss 0.0\n",
      "New data, epoch 13\n",
      "Step 0 Loss 0.0\n",
      "Step 100 Loss 0.0\n",
      "Step 200 Loss 0.0\n",
      "Step 300 Loss 0.0\n",
      "Step 400 Loss 0.0\n",
      "Step 500 Loss 0.0\n",
      "Step 600 Loss 0.0\n",
      "New data, epoch 14\n",
      "Step 0 Loss 0.0\n",
      "Step 100 Loss 0.0\n",
      "Step 200 Loss 0.0\n",
      "Step 300 Loss 0.0\n",
      "Step 400 Loss 0.0\n",
      "Step 500 Loss 0.0\n",
      "Step 600 Loss 0.0\n",
      "New data, epoch 15\n",
      "Step 0 Loss 0.0\n",
      "Step 100 Loss 0.0\n",
      "Step 200 Loss 0.0\n",
      "Step 300 Loss 0.0\n",
      "Step 400 Loss 0.0\n",
      "Step 500 Loss 0.0\n",
      "Step 600 Loss 0.0\n",
      "New data, epoch 16\n",
      "Step 0 Loss 0.0\n",
      "Step 100 Loss 0.0\n",
      "Step 200 Loss 0.0\n",
      "Step 300 Loss 0.0\n",
      "Step 400 Loss 0.0\n",
      "Step 500 Loss 0.0\n",
      "Step 600 Loss 0.0\n",
      "New data, epoch 17\n",
      "Step 0 Loss 0.0\n",
      "Step 100 Loss 0.0\n",
      "Step 200 Loss 0.0\n",
      "Step 300 Loss 0.0\n",
      "Step 400 Loss 0.0\n",
      "Step 500 Loss 0.0\n",
      "Step 600 Loss 0.0\n",
      "New data, epoch 18\n",
      "Step 0 Loss 0.0\n",
      "Step 100 Loss 0.0\n",
      "Step 200 Loss 0.0\n",
      "Step 300 Loss 0.0\n",
      "Step 400 Loss 0.0\n",
      "Step 500 Loss 0.0\n",
      "Step 600 Loss 0.0\n",
      "New data, epoch 19\n",
      "Step 0 Loss 0.0\n",
      "Step 100 Loss 0.0\n",
      "Step 200 Loss 0.0\n",
      "Step 300 Loss 0.0\n",
      "Step 400 Loss 0.0\n",
      "Step 500 Loss 0.0\n",
      "Step 600 Loss 0.0\n",
      "New data, epoch 20\n",
      "Step 0 Loss 0.0\n",
      "Step 100 Loss 0.0\n",
      "Step 200 Loss 0.0\n",
      "Step 300 Loss 0.0\n",
      "Step 400 Loss 0.0\n",
      "Step 500 Loss 0.0\n",
      "Step 600 Loss 0.0\n",
      "New data, epoch 21\n",
      "Step 0 Loss 0.0\n",
      "Step 100 Loss 0.0\n",
      "Step 200 Loss 0.0\n",
      "Step 300 Loss 0.0\n",
      "Step 400 Loss 0.0\n",
      "Step 500 Loss 0.0\n",
      "Step 600 Loss 0.0\n",
      "New data, epoch 22\n",
      "Step 0 Loss 0.0\n",
      "Step 100 Loss 0.0\n",
      "Step 200 Loss 0.0\n",
      "Step 300 Loss 0.0\n",
      "Step 400 Loss 0.0\n",
      "Step 500 Loss 0.0\n",
      "Step 600 Loss 0.0\n",
      "New data, epoch 23\n",
      "Step 0 Loss 0.0\n",
      "Step 100 Loss 0.0\n",
      "Step 200 Loss 0.0\n",
      "Step 300 Loss 0.0\n",
      "Step 400 Loss 0.0\n",
      "Step 500 Loss 0.0\n",
      "Step 600 Loss 0.0\n",
      "New data, epoch 24\n",
      "Step 0 Loss 0.0\n",
      "Step 100 Loss 0.0\n",
      "Step 200 Loss 0.0\n",
      "Step 300 Loss 0.0\n",
      "Step 400 Loss 0.0\n",
      "Step 500 Loss 0.0\n",
      "Step 600 Loss 0.0\n",
      "New data, epoch 25\n",
      "Step 0 Loss 0.0\n",
      "Step 100 Loss 0.0\n",
      "Step 200 Loss 0.0\n",
      "Step 300 Loss 0.0\n",
      "Step 400 Loss 0.0\n",
      "Step 500 Loss 0.0\n",
      "Step 600 Loss 0.0\n",
      "New data, epoch 26\n",
      "Step 0 Loss 0.0\n",
      "Step 100 Loss 0.0\n",
      "Step 200 Loss 0.0\n",
      "Step 300 Loss 0.0\n",
      "Step 400 Loss 0.0\n",
      "Step 500 Loss 0.0\n",
      "Step 600 Loss 0.0\n",
      "New data, epoch 27\n",
      "Step 0 Loss 0.0\n",
      "Step 100 Loss 0.0\n",
      "Step 200 Loss 0.0\n",
      "Step 300 Loss 0.0\n",
      "Step 400 Loss 0.0\n",
      "Step 500 Loss 0.0\n",
      "Step 600 Loss 0.0\n",
      "New data, epoch 28\n",
      "Step 0 Loss 0.0\n",
      "Step 100 Loss 0.0\n",
      "Step 200 Loss 0.0\n",
      "Step 300 Loss 0.0\n",
      "Step 400 Loss 0.0\n",
      "Step 500 Loss 0.0\n",
      "Step 600 Loss 0.0\n",
      "New data, epoch 29\n",
      "Step 0 Loss 0.0\n",
      "Step 100 Loss 0.0\n",
      "Step 200 Loss 0.0\n",
      "Step 300 Loss 0.0\n",
      "Step 400 Loss 0.0\n",
      "Step 500 Loss 0.0\n",
      "Step 600 Loss 0.0\n",
      "New data, epoch 30\n",
      "Step 0 Loss 0.0\n",
      "Step 100 Loss 0.0\n",
      "Step 200 Loss 0.0\n",
      "Step 300 Loss 0.0\n",
      "Step 400 Loss 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-9317531f58eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0mbatchX_placeholder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatchX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                     \u001b[0mbatchY_placeholder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatchY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                     \u001b[0minit_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0m_current_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 })\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m     \u001b[0mfetch_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \"\"\"\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m       \u001b[0;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0munique_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_uniquify_fetches\u001b[0;34m(fetch_mappers)\u001b[0m\n\u001b[1;32m    318\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseen_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m         \u001b[0mseen_fetches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m         \u001b[0munique_fetches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m       \u001b[0mm_value_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    470\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;31m# Necessary to support Python's collection membership operators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "#Step 3 Training the network\n",
    "with tf.Session() as sess:\n",
    "    #we stupidly have to do this everytime, it should just know\n",
    "    #that we initialized these vars. v2 guys, v2..\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    #interactive mode\n",
    "    plt.ion()\n",
    "    #initialize the figure\n",
    "    plt.figure()\n",
    "    #show the graph\n",
    "    plt.show()\n",
    "    #to show the loss decrease\n",
    "    loss_list = []\n",
    "\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        #generate data at eveery epoch, batches run in epochs\n",
    "        x,y = generateData()\n",
    "        #initialize an empty hidden state\n",
    "        _current_state = np.zeros((batch_size, state_size))\n",
    "\n",
    "        print(\"New data, epoch\", epoch_idx)\n",
    "        #each batch\n",
    "        for batch_idx in range(num_batches):\n",
    "            #starting and ending point per batch\n",
    "            #since weights reoccuer at every layer through time\n",
    "            #These layers will not be unrolled to the beginning of time, \n",
    "            #that would be too computationally expensive, and are therefore truncated \n",
    "            #at a limited number of time-steps\n",
    "            start_idx = batch_idx * truncated_backprop_length\n",
    "            end_idx = start_idx + truncated_backprop_length\n",
    "\n",
    "            batchX = x[:,start_idx:end_idx]\n",
    "            batchY = y[:,start_idx:end_idx]\n",
    "            \n",
    "            #run the computation graph, give it the values\n",
    "            #we calculated earlier\n",
    "            _total_loss, _train_step, _current_state, _predictions_series = sess.run(\n",
    "                [total_loss, train_step, current_state, predictions_series],\n",
    "                feed_dict={\n",
    "                    batchX_placeholder:batchX,\n",
    "                    batchY_placeholder:batchY,\n",
    "                    init_state:_current_state\n",
    "                })\n",
    "\n",
    "            loss_list.append(_total_loss)\n",
    "\n",
    "            if batch_idx%100 == 0:\n",
    "                print(\"Step\",batch_idx, \"Loss\", _total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
