{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approaches for Sentiment Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Lexicon Based Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 : Convert given text to smaller tokens (process called Tokenisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 : Count the number of times each token shows up (this tally is called Bag of Words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 : Check the subjectivity of each word from an existing sentiment lexicon(database of pre recorded sentiments of words) made by the researchers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4 : Once we have subjectivity of the words we can add them up to get the overall subjectivity of the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Machine Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 : Labelled available texts as positive or negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 : Apply Machine Learning classifier to learn the pattern and then predict the sentiment for unknown labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which one is better ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lexicons seems good but they lack the hidden meaning such as sarcasm as they only add up the subjectivity of separate tokens forming that text rather than finding the overall subjectivity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Neural Nets can understand the hidden meanings. Since they don't analyze text at the face value instead they make abstract representations of what they learn. These representations are called vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical,pad_sequences\n",
    "from tflearn.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imdb dataset loading\n",
    "train, test, a = imdb.load_data(path='imdb.pkl',n_words=10000,valid_portion=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainX,trainY = train\n",
    "testX,testY = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data Preprocessing\n",
    "#Sequence padding of inputs\n",
    "trainX = pad_sequences(trainX,maxlen=100,value=0.0)\n",
    "testX = pad_sequences(testX,maxlen=100,value=0.0)\n",
    "#converting labels to binary vectors\n",
    "trainY = to_categorical(trainY,nb_classes=2)\n",
    "testY = to_categorical(testY,nb_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Neural Network building\n",
    "net = tflearn.input_data([None,100])\n",
    "net = tflearn.embedding(net,input_dim=10000,output_dim=128)\n",
    "net = tflearn.lstm(net,128,dropout=0.8)\n",
    "net = tflearn.fully_connected(net,2,activation='softmax')\n",
    "net = tflearn.regression(net,optimizer='adam',learning_rate=0.001,loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 107  | total loss: \u001b[1m\u001b[32m0.68016\u001b[0m\u001b[0m | time: 59.019s\n",
      "\u001b[2K\r",
      "| Adam | epoch: 001 | loss: 0.68016 - acc: 0.5761 -- iter: 03424/22500\n"
     ]
    }
   ],
   "source": [
    "model = tflearn.DNN(net,tensorboard_verbose=0)\n",
    "model.fit(trainX,trainY,validation_set=(testX,testY),show_metric=True,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
